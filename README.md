# OpenAudit-Reasoner ğŸ”ğŸ¤–  
Multiâ€‘agent LLM reasoning orchestrator for transparent, auditable AI decisions.

> Compare how different AI agents think, not just what they answer.

***

## âœ¨ Overview

OpenAuditâ€‘Reasoner is a webâ€‘based platform that turns a single LLM into a team of specialized reasoning agents (Fast, Careful, Creative, Critical). For every user query, the system:

- Runs multiple agents in parallel.
- Captures stepâ€‘byâ€‘step â€œchainâ€‘ofâ€‘thoughtâ€ reasoning from each agent.
- Visualizes the reasoning paths sideâ€‘byâ€‘side.
- Optionally runs an automated verifier over the outputs.

The goal is to move LLMs from opaque â€œblack boxesâ€ to transparent, controllable collaboratorsâ€”especially for research, education, and safetyâ€‘critical domains.

***

## ğŸ§  Core Ideas

- Multiâ€‘agent LLM orchestration instead of a single, monolithic assistant.
- Transparent chainâ€‘ofâ€‘thought: users see how each agent arrived at its answer.
- Diversity by design: Fast vs Careful vs Creative vs Critical perspectives.
- Auditability: logs, verifier analysis, and reproducible runs for every query.
- Localâ€‘first: uses a model served via Ollama on your own machine (no cloud keys required).

***

## ğŸ—ï¸ Architecture at a Glance

- Frontend:  
  - Next.js + React SPA for interactive comparison UI.
  - Agents list, prompt input, sliders for randomness, and results grid.

- Backend:  
  - FastAPI orchestrator in Python.  
  - Async HTTP calls to the local LLM server (Ollama).  
  - Encapsulates agent personas, prompts, and parameters.

- Model Serving:  
  - Ollama running locally ( Llama 3 or another compatible model).  
  - Backend talks to `http://localhost:11434` by default.

- Data / Logging:  
  - Structured objects for Query â†’ AgentConfig â†’ AgentResponse â†’ VerifierResult.  
  - Ready to plug into a database if you want persistent audit logs.

***

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.10+  
- Node.js 18+  
- Git  
- Ollama installed locally and a model pulled (for example, `llama3`)

```bash
# Example: install and start Ollama, then pull a model
# (follow official Ollama docs first)
ollama pull llama3
ollama serve
```

### 2. Clone the Repository

```bash
git clone https://github.com/np-helios/multiagent-llm-reasoner.git
cd multiagent-llm-reasoner   # or your repo name
```

### 3. Backend Setup (FastAPI)

Create and activate a virtual environment, then install dependencies:

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt   # or: pip install fastapi uvicorn httpx pydantic python-dotenv
```

Start the backend server:

```bash
uvicorn main:app --reload --port 8000
```

### 4. Frontend Setup (Next.js)

```bash
cd frontend
npm install
npm run dev
```

The frontend will start on `http://localhost:3000`.

***

## ğŸ•¹ï¸ Using the App

1. Start Ollama (`ollama serve`) and ensure your model is available.  
2. Start the FastAPI backend (`uvicorn main:app --reload --port 8000`).  
3. Start the Next.js frontend (`npm run dev`).  
4. Open `http://localhost:3000` in your browser.

Youâ€™ll see:

- Agent list on the left (Fast, Careful, Creative, Critical).
- Prompt box in the center (e.g., â€œWhat is the square root of 6?â€).
- Controls for randomness / temperature.
- A â€œCompareâ€ button to trigger all agents at once.
- Result cards showing stepâ€‘byâ€‘step reasoning from each agent.
- Verifier panel and decision log at the bottom.

***

## ğŸ¤– Agent Personas

Each agent is just a different system prompt + parameter configuration:

- âš¡ Fast  
  - Short, efficient answers.  
  - Low temperature, low verbosity.

- âœ… Careful  
  - Slow, stepwise reasoning.  
  - Doubleâ€‘checks logic and details.

- ğŸ¨ Creative  
  - Exploratory, ideaâ€‘driven.  
  - Higher temperature, openâ€‘ended suggestions.

- ğŸ•µï¸ Critical  
  - Focuses on flaws, edge cases, and risks.  
  - Challenges assumptions and points out uncertainties.

These personas are defined in `agents.py` (or similar) and can be customized or extended with new roles (e.g., â€œDomainExpertâ€, â€œSkepticâ€, â€œTeacherâ€).

***

## ğŸ§ª Example Scenarios

Try prompts like:

- â€œExplain the square root of 6 to a 10â€‘yearâ€‘old.â€  
- â€œShould hospitals rely on AI for triage decisions? Why or why not?â€  
- â€œPropose three improvements to this project idea: [paste text].â€

Youâ€™ll see how different agents:

- Emphasize different aspects of the problem.  
- Offer alternative reasoning paths.  
- Sometimes disagreeâ€”highlighting ambiguity or risk.

This diversity is the whole point: **disagreement surfaces uncertainty**.

***

## ğŸ§¾ Project Structure (High Level)

```bash
.
â”œâ”€â”€ main.py          # FastAPI entrypoint / routes
â”œâ”€â”€ agents.py        # Agent persona definitions & orchestration logic
â”œâ”€â”€ pyproject.toml   # Backend dependencies
â”œâ”€â”€ venv/            # Python virtual env (ignored in git)
â”œâ”€â”€ frontend/        # Next.js app (UI)
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md        # You are here ğŸ™‚
```

***

## ğŸ›¡ï¸ Trust, Safety, and Auditability

OpenAuditâ€‘Reasoner is designed for **transparent AI**:

- Multiâ€‘path reasoning makes it easier to spot hallucinations.  
- Verifier module (where implemented) tags agent outputs as â€œvalid / needs reviewâ€.  
- Decision logs record prompt, agents used, parameters, and timestamps for every run.

This makes the system suitable as a **research tool** or a **teaching tool** for:

- Understanding LLM behavior.  
- Exploring interpretability and controllability.  
- Demonstrating AI risk and mitigation strategies.

***

## ğŸ§© Configuration

Key settings (model name, backend URL, ports, etc.) can be configured via environment variables:

- `OLLAMA_BASE_URL` â€“ default `http://localhost:11434`  
- `MODEL_NAME` â€“ e.g., `llama3`  
- `BACKEND_PORT` â€“ default `8000`

Create a `.env` file in the backend directory:

```env
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3
```

***

